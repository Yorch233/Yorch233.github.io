import{_ as l,r as d,a as p,b as n,d as o,e,t,l as u,h,i as m,F as g,f,v as b}from"./index-DbaR0l8g.js";const y={class:"home-publications fade-in"},_={class:"pub-card"},v={class:"pub-meta"},S={class:"pub-year"},w={class:"pub-venue"},x={class:"pub-title"},B={class:"pub-summary-text"},E={class:"pub-tags"},k={__name:"HomePublications",setup(D){const i={title:"Regularized Schrödinger Bridge via Distortion-Perception Perturbation for High-Fidelity Speech Enhancement",venue:"In Submission",year:"2025",tags:["Speech enhancement","Schrödinger bridge","Diffusion models","Distortion-perception tradeoff","Exposure bias"],summary:"Speech enhancement (SE) requires high-fidelity reconstruction of clean speech that preserves linguistic and paralinguistic cues while maintaining high perceptual quality. Recently, Schrödinger Bridge (SB), a family of diffusion-based generative models, has advanced SE by bridging degraded and clean speech distributions in a principled formulation, enabling higher-quality reconstructions with fewer sampling steps. However, diffusion-based SE methods still face two challenges:  (1) the fidelity-realism tradeoff, where they often prioritize perceptual realism encouraged by the learned speech prior, at the expense of fidelity; (2) the exposure bias issue, where iterative multi-step sampling causes early-step prediction errors to accumulate along the sampling trajectory and degrade enhanced speech quality. In this paper, we analyze standard SB training and show that it induces a systematic prediction drift, which biases the multi-step trajectory and amplifies error accumulation. To address this, we propose Regularized Schrödinger Bridge (RSB) for high-fidelity SE, a generative approach that reconciles fidelity and realism while mitigating exposure bias. RSB regularizes training with a Distortion-Perception Perturbation that constructs time-varying targets by interpolating between clean speech and posterior-mean estimates, and trains the network on perturbed intermediate states to correct toward the ground truth progressively. Consequently, such perturbation simulates inference-time prediction errors, mitigating the training–inference mismatch and thereby reducing exposure bias. Furthermore, it also injects posterior-mean estimates as fidelity-preserving guidance, facilitating reconstruction fidelity. Experiments on the WSJ0 corpus and VoiceBank+DEMAND dataset demonstrate that RSB improves reconstruction fidelity over the advanced SE baselines, yielding a favorable fidelity-realism tradeoff and reducing exposure bias."},a=d(!1);return(R,s)=>{const c=p("router-link");return n(),o("div",y,[e("article",_,[e("div",v,[e("span",S,t(i.year),1),e("span",w,t(i.venue),1)]),e("h2",x,[u(c,{class:"pub-link",to:"/RSB"},{default:h(()=>[b(t(i.title),1)]),_:1})]),e("p",{class:m(["pub-summary",{"is-collapsed":!a.value}])},[s[1]||(s[1]=e("span",{class:"pub-summary-label"},"Abstract",-1)),e("span",B,t(i.summary),1)],2),e("button",{class:"pub-expand",type:"button",onClick:s[0]||(s[0]=r=>a.value=!a.value)},t(a.value?"Collapse":"Expand"),1),e("div",E,[(n(!0),o(g,null,f(i.tags,r=>(n(),o("span",{key:r,class:"pub-tag"},t(r),1))),128))])])])}}},C=l(k,[["__scopeId","data-v-e8a7b543"]]);export{C as default};
